> **상태의 개수가 무수히 많은 커다란 MDP를 풀기 위해서는 문제 공간을 효율적으로 다뤄야 한다.**
>> - 함수를 활용한 근사
>> - 뉴럴넷
>> - 딥러닝

## 1. 함수를 활용한 근사

### ① 연속적인 상태 공간
- 상태의 값이 이산적으로 딱 맞아 떨어지지 않고, 실수 범위 내에서 연속적인 값을 가지는 상황
---


### ② 함수의 활용
$$ f(x) = ax+b$$
테이블에 상태와 상태 가치를 저장하듯, 함수에 상태와 상태가치를 저장한다.

- 속도가 100 km/h인 상태 $s_0$의 가치는 +1이고, 속도가 200 km/h인 상태 $s_1$의 가치는 -10이라면, 우리는 함수에 (100, 1)과 (200, -10)을 저장하고 싶다.
- 그렇다면 파라미터 $a, b$가 결정되게 된다.
- 함수를 활용한다면, **상태와 가치에 따라 함수의 파라미터를 수정**해나가며 학습을 진행할 것이다.

> $f(x)$는 상태 값을 입력으로 받아 가치 값을 내놓는 함수이므로 가치 함수라고 봐두 무방하다.

#### (1) 하지만, 우리가 가진 데이터가 2개보다 많다면??
- 파라미터 $a, b$를 결정할 수 없는 상황이 되어 버린다.
- 최소한 어느 한 점은 $f(x)$ 위에 있지 않게 된다.

#### (2) 최소 제곱법의 활용
- 오차의 제곱의 합을 최소화하는 파라미터 $a,b$를 찾는 방법론
- 함수의 곡선이 데이터에 가깝게 지나가도록 피팅하는 방법
> 오차란, 실제 데이터와 $f(x)$에 상태를 대입한 값의 차이를 말한다.

#### (3) 함수를 피팅한다?
- 함수에 데이터를 기록한다.
- 데이터 점들을 가장 가깝게 지나가도록 함수를 그린다.
- 함수 $f$의 파라미터 $(a_0$ ~ $a_n)$의 값을 찾는다.
- 함수 $f$를 학습한다.

#### (4) 노이즈의 본질적 의미
- 데이터에는 여러 확률적 요소가 끼어 있고, 그로 인해 실제 구하고자 하는 값과 각각의 데이터는 각각 다른 값을 가지게 된다.
---
### ③ 오버 피팅 (Over Fitting)
- $f$를 정할 때, 너무 유연한 함수(차수가 높은 함수)를 사용하여 $f$가 노이즈에 피팅해 버리는 것을 가리키는 말
---
### ④ 언더 피팅 (Under Fitting)
- 실제 모델을 담기에 함수 $f$의 유연성이 부족하여 주어진 데이터와의 오차가 큰 상황을 가리키는 말
---
### ⑤ 함수의 장점 :: 일반화
- 모든 상태 $s$에 대해 별도의 저장 공간이 필요하지 않다는 장점을 가진다.
- 일반화를 잘 하도록 함수 $f$를 학습시킨다면 가보지 않은 상태에 대해서도 그 가치를 가늠할 수 있게 된다.
- 함수를 활용하여 학습을 하게 된다면, 결과물을 저장하는데 필요한 용량이 매우 줄어들 뿐만 아니라, 각각의 데이터를 기억하는 것이 아닌 함수의 파라미터만 기억하면 된다는 이점을 가져다 준다.


> **"함수를 통해서 값을 기록한다." = "함수의 파라미터를 찾아낸다."**
> - 함수의 파라미터를 찾아내는 경우, 새로운 데이터가 와도 해당 가치를 알아낼 수 있다.
> - **Q.** 어떤 함수를 써야 할까?
> - **A.** 인공 신경망을 활용해야 한다. 