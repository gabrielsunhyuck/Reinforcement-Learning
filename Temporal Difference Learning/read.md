# 몬테카를로 vs. 시간차(Temporal Difference) 학습

## 1. 몬테카를로 방식의 단점

* **업데이트 시점의 제약**: 업데이트를 하려면 에피소드가 끝날 때까지 기다려야 합니다.
* **리턴 값의 필요성**: 업데이트를 위해서는 리턴($G_t$)이 필요한데, 이 값은 에피소드 종료 전까지는 알 수 없습니다.
    * 👉 **적용 환경 제한**: 에피소드 종료가 보장되는 MDP(Markov Decision Process)에서만 사용이 가능합니다.

> ● 몬테카를로 방식은 리턴의 기댓값을 통해 가치를 계산합니다.\
● 리턴은 하나의 에피소드가 끝날 때 얻어지는 총 보상입니다.\
● 따라서 에피소드가 끝나기 전에 업데이트하는 방법이 필요합니다.

---

## 2. 시간차(Temporal Difference) 학습

시간적인 차이(Temporal Difference)를 이용해 밸류를 업데이트하는 방식입니다. **미래의 추측으로 과거의 추측을 업데이트**하는 것이 핵심입니다.

### 시간차 타겟 (Temporal Difference Target)

$$r_{t+1} + \gamma v_\pi (s_{t+1})$$

* 이 식은 벨만 기대 방정식에서 파생되었으며, 시간차 학습의 근간이 됩니다.

### 시간차 알고리즘

1.  **테이블 초기화**
2.  **경험 쌓기**: 에이전트가 환경과 상호작용하며 경험을 쌓습니다.
3.  **테이블 업데이트**: 에피소드가 끝나기 전에, 각 상태 전이(state transition)가 일어날 때마다 즉시 밸류를 업데이트할 수 있습니다.

**업데이트 식:**
$$V(s_t) \leftarrow V(s_t) + \alpha [r_{t+1} + \gamma V(s_{t+1}) - V(s_t)]$$

* **몬테카를로 방식**의 업데이트 식($V(s_t) \leftarrow V(s_t) + \alpha (G_t - V(s_t))$)에서 **리턴($G_t$)을** **시간차 타겟($r_{t+1} + \gamma V(s_{t+1})$)으로 대체**하여 계산합니다.

---