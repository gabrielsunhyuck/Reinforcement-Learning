# 몬테카를로 vs. 시간차(Temporal Difference) 학습

### [ 학습 시점 ]   
- 몬테카를로는 에피소드가 끝나고 보상이 정해져야 비로소 되돌아보면서 학습을 진행할 수 있습니다.   
> **Episodic MDP :** 바둑, 스타크래프트 등 종료 조건이 명확한 경우에 적용
- 시간차 학습은 한 스텝이 끝날 때마다 바로바로 테이블의 값들을 업데이트할 수 있습니다.   
> **Non-Episodic MDP :** 주식 시장에서의 포트폴리오 분배처럼 명확한 종료 조건이 없거나, 하나의 에피소드가 너무 길어지는 경우에 적용 **(어떤 MDP에도 적용할 수 있다는 장점을 가집니다.)**

### [ 편향성 ]
- 몬테카를로는 가치 함수의 정의가 리턴의 기댓값이므로, 리턴의 평균을 위한 업데이트 방식입니다.   
> **편향되지 않은 안전한 방법론**
- 시간차 학습은 다음 상태와 현재 추측치 사이 차이를 줄여주는 방향으로 업데이트하는 방식입니다.
> **Temporal Difference 타깃은 편향됨 (실제 가치와 가까워지리라는 보장이 없음)**$$V(s_{t+1}) \not= v_\pi (s_{t+1})$$

### [ 분산 ]
- 몬테카를로 방식은 에피소드 하나가 끝나면서 리턴을 얻고, 그를 통해 업데이트가 이루집니다.   
> 수많은 상태 전이와 정책으로 이루어져 있기 때문에, 에피소드의 길이가 다양함 **(분산, 변동성이 큼)**   
- 시간차 학습은 한 샘플만을 통해 바로 업데이트가 가능합니다.   
> **값들이 평균 근처에 몰려있고, 분산이 작음**

----
## 1. 몬테카를로 방식의 단점

* **업데이트 시점의 제약**: 업데이트를 하려면 에피소드가 끝날 때까지 기다려야 합니다.
* **리턴 값의 필요성**: 업데이트를 위해서는 리턴($G_t$)이 필요한데, 이 값은 에피소드 종료 전까지는 알 수 없습니다.
    * 👉 **적용 환경 제한**: 에피소드 종료가 보장되는 MDP(Markov Decision Process)에서만 사용이 가능합니다.

> ● 몬테카를로 방식은 리턴의 기댓값을 통해 가치를 계산합니다.\
● 리턴은 하나의 에피소드가 끝날 때 얻어지는 총 보상입니다.\
● 따라서 에피소드가 끝나기 전에 업데이트하는 방법이 필요합니다.

---

## 2. 시간차(Temporal Difference) 학습

시간적인 차이(Temporal Difference)를 이용해 밸류를 업데이트하는 방식입니다. **미래의 추측으로 과거의 추측을 업데이트**하는 것이 핵심입니다.

> 학습의 변동성이 작기 때문에 큰 폭의 업데이트가 가능하여 학습률 (Learing rate)를 몬테카를로 방식에 비해 크게 설정할 수 있습니다.

### 시간차 타겟 (Temporal Difference Target)

$$r_{t+1} + \gamma v_\pi (s_{t+1})$$

* 이 식은 벨만 기대 방정식에서 파생되었으며, 시간차 학습의 근간이 됩니다.

### 시간차 알고리즘

1.  **테이블 초기화**
2.  **경험 쌓기**: 에이전트가 환경과 상호작용하며 경험을 쌓습니다.
3.  **테이블 업데이트**: 에피소드가 끝나기 전에, 각 상태 전이(state transition)가 일어날 때마다 즉시 밸류를 업데이트할 수 있습니다.

**업데이트 식:**
$$V(s_t) \leftarrow V(s_t) + \alpha [r_{t+1} + \gamma V(s_{t+1}) - V(s_t)]$$

* **몬테카를로 방식**의 업데이트 식($V(s_t) \leftarrow V(s_t) + \alpha (G_t - V(s_t))$)에서 **리턴($G_t$)을** **시간차 타겟($r_{t+1} + \gamma V(s_{t+1})$)으로 대체**하여 계산합니다.

---