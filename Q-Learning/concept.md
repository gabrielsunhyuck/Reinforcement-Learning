#### **\[ Human-level Control through Deep Reinforcement Learning - NATURE\]**

[https://www.nature.com/articles/nature14236](https://www.nature.com/articles/nature14236)

:: 딥러닝과 강화 학습의 Q러닝을 결합한 Deep Q-Learning이라는 알고리즘을 통해 고전 비디오 게임인 아타리 2600을 사람 수준으로 플레이할 수 있도록 에이전트를 학습시킨 연구

---

## **On-Policy vs. Off-Policy**  

-   직접 게임을 하는 민혁 :: On-Policy
-   민혁이가 게임하는 것을 구경하는 찬혁 :: Off-Policy

On-Policy는 타깃 정책과 행동 정책이 같은 경우, 즉, 직접 경험을 통해 학습하는 것을 말하고,  
Off-Policy는 타깃 정책과 행동 정책이 다른 경우, 즉, 간접 경험을 통해 학습하는 것을 말한다.

-   타깃 정책 (Target policy) : 강화하고자 하는 목표가 되는 정책
-   행동 정책 (Behavior policy) : 실제로 환경과 상호 작용하며 경험을 쌓고 있는 정책

---

## **Off-Policy의 장점**

-   과거의 경험을 재사용할 수 있다.  
    \- 학습이 진행되면서 정책이 업데이트 되더라도 이전에 경험했던 샘플들을 그대로 재사용할 수 있다.  
    \- 데이터 효율성 측면에서 큰 장점이 될 수 있다.
-   사람의 데이터로부터 학습할 수 있다.  
    \- 행동 정책은 환경과 상호작용하며 경험을 쌓을 수 있다면 어떤 것이든 상관이 없다.  
    \- 전문가가 만들어냈던 양질의 데이터를 학습에 사용하여 학습 초기의 무의미한 행동을 하는 단계를 빠르게 벗어날 수 있다.
-   일대다, 다대일 학습이 가능하다.  
    \- 1개의 정책만을 경험에 쌓게 두고, 그로부터 생성된 데이터를 이용해 동시에 여러 개의 정책을 학습시킬 수 있다.  
    \- 동시에 여러 개의 정책이 겪은 데이터를 모아서 1개의 정책을 업데이트할 수도 있다.

---

## **Q-Learning의 이론적 배경**

#### **(1) 벨만 최적 방정식**

우리의 목적은 최적의 액션-가치 함수인 $q_*$를 찾는 것이다.  
이 최적 액션-가치 함수를 찾는 것의 해답은 벨만 최적 방정식에 있다.

$$q_*(s, a)=r_s^a + \gamma \sum_{s' \in S} P_{ss'}^a \max_{a'} q_*(s', a')$$

벨만 최적 방정식의 0번째 iteration에서의 수식은 아래와 같다.

$$q_*(s, a) = \mathbb{E}_{s'} [r + \gamma \max_{a'} q_* (s', a')]$$

$r + \gamma max_{a'} q_* (s', a')$를 정답이라고 보고 그 방향으로 조금씩 업데이트해 나간다.

#### **(2) SARSA vs. Q-Learning**

**\- SARSA**  
$$ Q(S, A) = Q(S, A) + \alpha (R + \gamma Q(S', A') - Q(S, A))$$

● 벨만 기대 방정식에 기원을 두고 있다.  
● 행동 정책과 타깃 정책이 모두 입실론-그리디 정책을 따른다.

**\- Q- Learning**  
$$ Q(S, A) = Q(S, A) + \alpha ( R + \gamma \max_{A'} Q(S', A') - Q(S, A))$$

● 벨만 최적 방정식에 기원을 두고 있다.  
● 행동 정책에는 탐험에 대한 행동 탐색이 이루어지는 입실론-그리디 정책을 따르는 반면, 타깃 정책에는 가장 Q 값이 높은 액션을 선택하는 방식인 그리디 정책을 따른다.

---
